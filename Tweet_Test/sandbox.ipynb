{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n",
    "from datetime import datetime\n",
    "from pythainlp import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import langdetect\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import schedule\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = glob.glob('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')  \n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        ##-------------------- twitter --------------------##\n",
    "        self._url = \"https://api.aiforthai.in.th/ssense\"                     \n",
    "        self._headers = {'Apikey': \"0kFkiFLdf4TAyY3JeUT9WVnB5naP6SjW\"}\n",
    "        consumer_key = \"EaFU9nJw2utR0lo2PUmJE3VZy\"\n",
    "        consumer_secret = \"DsZuVw0tEl6GHhyK08tunsOE9ICSfwplEhRDMQwB8VIqngZ6i8\"\n",
    "        access_token = \"759317188863897600-nuwQmcYfDX8lvdRyw2eCD6fMRMkLzzZ\"\n",
    "        access_token_secret = 'zFFc5OJywNMBrRAblI7kFV62ZTZPHfTU1Q5kZ1cKzUupD'\n",
    "        auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        self._api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "        self._url = \"https://api.aiforthai.in.th/ssense\"                     \n",
    "        self._headers = {'Apikey': \"0kFkiFLdf4TAyY3JeUT9WVnB5naP6SjW\"}\n",
    "\n",
    "        self.keys = []\n",
    "        self.df = None\n",
    "        self._start = 0\n",
    "\n",
    "    def getSentimentENG(self,text):\n",
    "        if TextBlob(text).sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif TextBlob(text).sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "\n",
    "    def getSentimentTH(self,text):\n",
    "        text = re.sub(r'[%]',' ',text)\n",
    "        params = {'text':text}\n",
    "        response = requests.get(self._url, headers=self._headers, params=params)\n",
    "        try:\n",
    "            polarity = str(response.json()['sentiment']['polarity'])\n",
    "        except (KeyError):\n",
    "            polarity = 'neutral'\n",
    "        return polarity\n",
    "\n",
    "    def formatdatetime(self,column):\n",
    "        self.df[column] = pd.to_datetime(self.df[column]).dt.strftime('%Y/%m/%d') #dmY ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á\n",
    "        self.df[column] = pd.to_datetime(self.df[column])\n",
    "    \n",
    "    def sortdf(self,columns):\n",
    "        self.df.sort_values(by=columns,inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def unionfile(self,filenames):              #type filename -> list\n",
    "        for file in filenames:\n",
    "            df1 = pd.read_csv(file)\n",
    "            if self._start != 0:\n",
    "                self.df = pd.concat([self.df,df1])\n",
    "                self.df.drop_duplicates(keep='last',inplace=True)\n",
    "            else:\n",
    "                self.df = df1\n",
    "                self._start += 1\n",
    "        return self.df\n",
    "    \n",
    "    def setnewDF(self,dataframe):\n",
    "        self.df = dataframe\n",
    "        return self.df\n",
    "\n",
    "    def getperiod(self,since,until):  ####column for twitter\n",
    "        self.formatdatetime('Time')\n",
    "        dff = self.df\n",
    "        dff.sort_values(by=['Time','Keyword'],inplace=True)\n",
    "        if since == None and until != None:\n",
    "            mask = (dff['Time']<=until)\n",
    "        elif since != None and until == None:\n",
    "            mask = (dff['Time']>=since)\n",
    "        elif since != None and until != None:\n",
    "            mask = (dff['Time']>=since) & (dff['Time']<=until)\n",
    "        else:\n",
    "            return\n",
    "        return dff.loc[mask]\n",
    "\n",
    "    def getrowwithkeys(self,keys):              #type keys -> list\n",
    "        df = self.df\n",
    "        return df.loc[df['Keyword'].isin(keys)]\n",
    "\n",
    "    def collectword(self,dataframe):            #before edit collect thai can not use\n",
    "        en_stops = set(stopwords.words('english'))\n",
    "        word = {}\n",
    "        for i in dataframe['Tweet']:    #only tweet\n",
    "            if langdetect.detect(i) != 'th':\n",
    "                allwords = i.split()\n",
    "                for w in allwords: \n",
    "                    if w not in en_stops:\n",
    "                        if w in word:\n",
    "                            word[w] += 1\n",
    "                        else:\n",
    "                            word[w] = 1\n",
    "            else:\n",
    "                allwords = word_tokenize(i, engine='newmm')\n",
    "                for w in allwords: \n",
    "                    if w not in en_stops:\n",
    "                        if w in word:\n",
    "                            word[w] += 1\n",
    "                        else:\n",
    "                            word[w] = 1\n",
    "        del word['RT']\n",
    "        del word[' ']   #for thai language\n",
    "        sortword = sorted(word.items(),key=lambda x:x[1],reverse=True)\n",
    "        return sortword     #tuple in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Language</th>\n",
       "      <th>Time</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>shounen ai</td>\n",
       "      <td>myrialune</td>\n",
       "      <td>zoebug71 Also my favouriteeeee shounen ai ever...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>shounen ai</td>\n",
       "      <td>MintFrogkie</td>\n",
       "      <td>abcdefghijklmvh cloudyyamilee SRY FOR LATE RES...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>shounen ai</td>\n",
       "      <td>AthyChii</td>\n",
       "      <td>Boy Meets Maria Completed Shounen Ai</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>shounen ai</td>\n",
       "      <td>mkn0135</td>\n",
       "      <td>RT AthyChii Boy Meets Maria Completed Shounen Ai</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>shounen ai</td>\n",
       "      <td>ruinedlunarian</td>\n",
       "      <td>The only Gundam poster I own and it looks like...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>ncwmqj</td>\n",
       "      <td>RT todojamy ‡πÄ‡∏ò‡∏£‡∏î‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏µ‡∏ß‡∏¥‡∏ß ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏Å‡∏µ‡∏¨‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠']</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4876</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>__RaInBoWLiGhT</td>\n",
       "      <td>RT btfeffectx ‡∏ú‡∏°‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ EIGHTYSIX ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏î‡∏π...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>lilinjeee</td>\n",
       "      <td>Teddix ‡∏´‡∏ô‡∏π‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Great pretender ‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏ô‡∏¥‡πÄ...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>ncwmqj</td>\n",
       "      <td>RT todojamy ‡πÄ‡∏ò‡∏£‡∏î‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏µ‡∏ß‡∏¥‡∏ß ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏Å‡∏µ‡∏¨‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠']</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>littleslothseiy</td>\n",
       "      <td>RT MWN ‡∏ñ‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£‡∏ä‡∏≠‡∏ö‡∏î‡∏π‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏ß‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>ùöñùöäùöîùöóùöäùöé ùöïùöíùöóùöé üê∞üê£üêª</td>\n",
       "      <td>[]</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4553 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keyword             User  \\\n",
       "4050   shounen ai        myrialune   \n",
       "4051   shounen ai      MintFrogkie   \n",
       "4061   shounen ai         AthyChii   \n",
       "4062   shounen ai          mkn0135   \n",
       "4063   shounen ai   ruinedlunarian   \n",
       "...           ...              ...   \n",
       "4738  ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥           ncwmqj   \n",
       "4876  ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥   __RaInBoWLiGhT   \n",
       "4877  ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥        lilinjeee   \n",
       "4878  ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥           ncwmqj   \n",
       "4902  ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥  littleslothseiy   \n",
       "\n",
       "                                                  Tweet Language       Time  \\\n",
       "4050  zoebug71 Also my favouriteeeee shounen ai ever...       en 2022-03-25   \n",
       "4051  abcdefghijklmvh cloudyyamilee SRY FOR LATE RES...       en 2022-03-25   \n",
       "4061               Boy Meets Maria Completed Shounen Ai       en 2022-03-25   \n",
       "4062   RT AthyChii Boy Meets Maria Completed Shounen Ai       en 2022-03-25   \n",
       "4063  The only Gundam poster I own and it looks like...       en 2022-03-25   \n",
       "...                                                 ...      ...        ...   \n",
       "4738  RT todojamy ‡πÄ‡∏ò‡∏£‡∏î‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏µ‡∏ß‡∏¥‡∏ß ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏Å‡∏µ‡∏¨‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢...       th 2022-03-26   \n",
       "4876  RT btfeffectx ‡∏ú‡∏°‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ EIGHTYSIX ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏î‡∏π...       th 2022-03-26   \n",
       "4877  Teddix ‡∏´‡∏ô‡∏π‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Great pretender ‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏ô‡∏¥‡πÄ...       th 2022-03-26   \n",
       "4878  RT todojamy ‡πÄ‡∏ò‡∏£‡∏î‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏µ‡∏ß‡∏¥‡∏ß ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏Å‡∏µ‡∏¨‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢...       th 2022-03-26   \n",
       "4902  RT MWN ‡∏ñ‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£‡∏ä‡∏≠‡∏ö‡∏î‡∏π‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏ß‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤...       th 2022-03-26   \n",
       "\n",
       "        User Location     Hashtag  Polarity  Likes  Retweet  Sentiment  \n",
       "4050              NaN          []   neutral      2        0       0.00  \n",
       "4051              NaN          []  negative      1        0      -0.30  \n",
       "4061              NaN          []   neutral     12        1       0.00  \n",
       "4062              NaN          []   neutral      0        1       0.00  \n",
       "4063              NaN          []  positive      0        0       0.30  \n",
       "...               ...         ...       ...    ...      ...        ...  \n",
       "4738              NaN  ['‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠']  positive      0       33      66.67  \n",
       "4876              NaN          []  negative      0        9      66.67  \n",
       "4877              NaN          []  negative      0        0      75.00  \n",
       "4878              NaN  ['‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠']  positive      0       33      66.67  \n",
       "4902  ùöñùöäùöîùöóùöäùöé ùöïùöíùöóùöé üê∞üê£üêª          []  positive      0     1006      66.67  \n",
       "\n",
       "[4553 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DataManager()\n",
    "#dm.unionfile(filename)\n",
    "dm.unionfile(['tweet_data_2732022.csv'])\n",
    "dm.getperiod('2022-03-25',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢', 424)\n",
      "('Pixar', 245)\n",
      "('‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ', 231)\n",
      "('‡∏ï‡∏≠‡∏ô', 198)\n",
      "('‡πÉ‡∏´‡∏°‡πà', 191)\n",
      "('‡∏ï‡πà‡∏≤‡∏á', 184)\n",
      "('shounen', 179)\n",
      "('‡πÇ‡∏•‡∏Å', 177)\n",
      "('‡πÄ‡∏î‡πá‡∏Å‡∏î‡∏µ', 132)\n",
      "('Algiz', 118)\n"
     ]
    }
   ],
   "source": [
    "df1 = dm.getrowwithkeys(['‡∏ï‡πà‡∏≤‡∏á‡πÇ‡∏•‡∏Å','pixar','shounen']).sort_values('Keyword')\n",
    "for word in dm.collectword(df1)[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in dm.collectword(df1)[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "filename = ['tweet_data_2032022.csv','tweet_data_1932022.csv','tweet_data_2132022_1.csv','tweet_data_2232022.csv']\n",
    "i = 0\n",
    "for file in filename:\n",
    "    df1 = pd.read_csv(file)\n",
    "    if i != 0:\n",
    "        df = pd.concat([df,df1])\n",
    "        df.drop_duplicates(keep='last',inplace=True)\n",
    "    else:\n",
    "        df = df1\n",
    "        i += 1\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%Y/%m/%d')\n",
    "#df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%d %m %Y')\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df.sort_values(by=['Time','Keyword'],inplace=True)\n",
    "\n",
    "#time = list(set(df['Time'].tolist()))\n",
    "mask = (df['Time']>='2022-03-19') & (df['Time']<='2022-03-21')\n",
    "#mask = (df['Time']>='19-03-2022') & (df['Time']<='21-03-2022')\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stops = set(stopwords.words('english'))\n",
    "filename = ['tweet_data_2732022.csv']\n",
    "i = 0\n",
    "for file in filename:\n",
    "    df1 = pd.read_csv(file)\n",
    "    if i != 0:\n",
    "        df = pd.concat([df,df1])\n",
    "        df.drop_duplicates(keep='last',inplace=True)\n",
    "    else:\n",
    "        df = df1\n",
    "        i += 1\n",
    "        \n",
    "dff = df.loc[df['Keyword'].isin(['pixar','‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞'])]\n",
    "word = {}\n",
    "\n",
    "for i in dff['Tweet']:\n",
    "    if langdetect.detect(i) != 'th':\n",
    "        allwords = i.split()\n",
    "        for w in allwords: \n",
    "            if w not in en_stops:\n",
    "                if w in word:\n",
    "                    word[w] += 1\n",
    "                else:\n",
    "                    word[w] = 1\n",
    "    else:\n",
    "        allwords = word_tokenize(i, engine='newmm')\n",
    "        for w in allwords: \n",
    "            if w not in en_stops:\n",
    "                if w in word:\n",
    "                    word[w] += 1\n",
    "                else:\n",
    "                    word[w] = 1\n",
    "del word['RT']\n",
    "del word[' ']\n",
    "sortword = sorted(word.items(),key=lambda x:x[1],reverse=True)\n",
    "for i in sortword[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "all_words = ['There', 'is', 'a', 'tree','near','the','river']\n",
    "for word in all_words: \n",
    "    if word not in en_stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "url = \"https://api.aiforthai.in.th/ssense\"\n",
    " \n",
    "text = '‡∏™‡∏≤‡∏Ç‡∏≤‡∏ô‡∏µ‡πâ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ'\n",
    " \n",
    "data = {'text':text}\n",
    " \n",
    "headers = {\n",
    "    'Apikey': \"0kFkiFLdf4TAyY3JeUT9WVnB5naP6SjW\"\n",
    "    }\n",
    " \n",
    "response = requests.post(url, data=data, headers=headers)\n",
    " \n",
    "print(response.json())\n",
    "response.json()['preprocess']['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(set(df['Keyword'].tolist()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
