{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n",
    "from datetime import datetime\n",
    "from pythainlp import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import langdetect\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import schedule\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "from pythainlp.corpus import thai_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = glob.glob('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')  \n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        ##-------------------- twitter --------------------##\n",
    "        self._url = \"https://api.aiforthai.in.th/ssense\"                     \n",
    "        self._headers = {'Apikey': \"0kFkiFLdf4TAyY3JeUT9WVnB5naP6SjW\"}\n",
    "        consumer_key = \"EaFU9nJw2utR0lo2PUmJE3VZy\"\n",
    "        consumer_secret = \"DsZuVw0tEl6GHhyK08tunsOE9ICSfwplEhRDMQwB8VIqngZ6i8\"\n",
    "        access_token = \"759317188863897600-nuwQmcYfDX8lvdRyw2eCD6fMRMkLzzZ\"\n",
    "        access_token_secret = 'zFFc5OJywNMBrRAblI7kFV62ZTZPHfTU1Q5kZ1cKzUupD'\n",
    "        auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        self._api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "        self._url = \"https://api.aiforthai.in.th/ssense\"                     \n",
    "        self._headers = {'Apikey': \"0kFkiFLdf4TAyY3JeUT9WVnB5naP6SjW\"}\n",
    "\n",
    "        self.keys = []\n",
    "        self.df = None\n",
    "        self._start = 0\n",
    "        self.filenames = []\n",
    "\n",
    "    def getSentimentENG(self,text):\n",
    "        if TextBlob(text).sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif TextBlob(text).sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "\n",
    "    def getSentimentTH(self,text):\n",
    "        text = re.sub(r'[%]',' ',text)\n",
    "        params = {'text':text}\n",
    "        response = requests.get(self._url, headers=self._headers, params=params)\n",
    "        try:\n",
    "            polarity = str(response.json()['sentiment']['polarity'])\n",
    "        except (KeyError):\n",
    "            polarity = 'neutral'\n",
    "        return polarity\n",
    "\n",
    "    def formatdatetime(self,column):\n",
    "        self.df[column] = pd.to_datetime(self.df[column]).dt.strftime('%Y/%m/%d') #dmY ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á\n",
    "        self.df[column] = pd.to_datetime(self.df[column])\n",
    "    \n",
    "    def sortdf(self,columns):\n",
    "        self.df.sort_values(by=columns,inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def unionfile(self,filenames):              #type filename -> list\n",
    "        self._start = 0\n",
    "        self.filenames = filenames\n",
    "        for file in filenames:\n",
    "            df1 = pd.read_csv(file)\n",
    "            if self._start != 0:\n",
    "                self.df = pd.concat([self.df,df1])\n",
    "                self.df.drop_duplicates(keep='last',inplace=True)\n",
    "            else:\n",
    "                self.df = df1\n",
    "                self._start += 1\n",
    "        self.keys = list(set(self.df['Keyword'].tolist()))\n",
    "        self.collectfile()\n",
    "        return self.df\n",
    "    \n",
    "    def setnewDF(self,dataframe):\n",
    "        self.df = dataframe\n",
    "        return self.df\n",
    "    \n",
    "    def setdefaultDF(self):\n",
    "        self.df = self.unionfile(self.filenames)\n",
    "        return self.df\n",
    "    \n",
    "    def collectfile(self):\n",
    "        self.df[\"Time\"] = pd.to_datetime(self.df[\"Time\"]).dt.strftime('%Y-%m-%d')\n",
    "        keys = list(set(df['Keyword'].tolist()))\n",
    "        folder = \"collectkeys\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)    \n",
    "        for key in keys:\n",
    "            path = str(folder+'/'+key)\n",
    "            dff = self.df.loc[self.df['Keyword'].isin([key])]\n",
    "            days = list(set(dff['Time'].tolist()))\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            for d in days:\n",
    "                dfff = dff.loc[dff['Time'].isin([d])]\n",
    "                dfff.to_csv(path+'/'+key+'_'+d+'.csv',encoding='utf-8',index=False)\n",
    "    \n",
    "    def newUnion(self,filenames):\n",
    "        self.filenames = filenames\n",
    "        path=os.getcwd()\n",
    "        keys = []\n",
    "        start = 0\n",
    "        for f in glob.glob(path+'/collectkeys/*'):\n",
    "            keyname = os.path.split(f)[-1]\n",
    "            keys.append(keyname)\n",
    "        for k in keys:\n",
    "            for file in glob.glob(path+'/collectkeys/'+k+'/*.csv'):\n",
    "                if start == 0:\n",
    "                    df = pd.read_csv(file)\n",
    "                    start +=1\n",
    "                else:\n",
    "                    dff = pd.read_csv(file)\n",
    "                    df = pd.concat([df,dff])\n",
    "        self.keys = keys\n",
    "        self.collectfile()\n",
    "        return self.df\n",
    "    \n",
    "    def getperiod(self,since,until):  ####column for twitter\n",
    "        self.formatdatetime('Time')\n",
    "        dff = self.df\n",
    "        dff.sort_values(by=['Time','Keyword'],inplace=True)\n",
    "        if since == None and until != None:\n",
    "            mask = (dff['Time']<=until)\n",
    "        elif since != None and until == None:\n",
    "            mask = (dff['Time']>=since)\n",
    "        elif since != None and until != None:\n",
    "            mask = (dff['Time']>=since) & (dff['Time']<=until)\n",
    "        else:\n",
    "            return\n",
    "        return dff.loc[mask]\n",
    "    \n",
    "    def getrowwithkeys(self,keys):              #type keys -> list\n",
    "        df = self.df\n",
    "        return df.loc[df['Keyword'].isin(keys)]\n",
    "\n",
    "    def collectwords(self,dataframe):\n",
    "        #print(dataframe)\n",
    "        dataframe = dataframe.reset_index()\n",
    "        th_stopwords = list(thai_stopwords())\n",
    "        en_stops = set(stopwords.words('english'))\n",
    "        word = {}\n",
    "        for index,row in dataframe.iterrows():    #only tweet\n",
    "            if row['Language'] == 'eng':\n",
    "                allwords = row['Tweet'].split()\n",
    "                for w in allwords: \n",
    "                    if w not in en_stops:\n",
    "                        if w in word:\n",
    "                            word[w] += 1\n",
    "                        else:\n",
    "                            word[w] = 1\n",
    "            elif row['Language'] == 'th':\n",
    "                allwords = word_tokenize(row['Tweet'], engine='newmm')\n",
    "                for w in allwords: \n",
    "                    if w not in th_stopwords:\n",
    "                        if w in word:\n",
    "                            word[w] += 1\n",
    "                        else:\n",
    "                            word[w] = 1\n",
    "            else:\n",
    "                pass\n",
    "        if 'RT' in word:\n",
    "            del word['RT']  #for twitter\n",
    "        if ' ' in word:\n",
    "            del word[' ']   #for thai language\n",
    "        sortword = sorted(word.items(),key=lambda x:x[1],reverse=True)\n",
    "        worddf = pd.DataFrame(sortword,columns=['Word','Count'])\n",
    "        return worddf   #word dataframe\n",
    "        #return sortword     #tuple in list\n",
    "    \n",
    "    def deletekeyword(self,keyword):\n",
    "        path=os.getcwd()\n",
    "        for k  in keyword:\n",
    "            shutil.rmtree(path+'//collectkeys//'+k+'//')\n",
    "            self.keys.remove(k)\n",
    "            self.df.drop(self.df[self.df['Keyword']==k].index,inplace = True)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Language</th>\n",
       "      <th>Time</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d animation</td>\n",
       "      <td>LinearUwu</td>\n",
       "      <td>RT blutack1966 The future will always be brigh...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2d animation</td>\n",
       "      <td>animationjobs</td>\n",
       "      <td>2D Background Artist job in Dicesol Animation ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Global üåç</td>\n",
       "      <td>[]</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2d animation</td>\n",
       "      <td>l3oAlways</td>\n",
       "      <td>RT irshema LGGGUYS ALWAYS TELL U SURE DEAL LET...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Poland üáµüá±</td>\n",
       "      <td>['LGG', 'websitedesign', 'GraphicDesign', 'vid...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2d animation</td>\n",
       "      <td>BiraboneyeVicky</td>\n",
       "      <td>RT irshema LGGGUYS ALWAYS TELL U SURE DEAL LET...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['LGG', 'websitedesign', 'GraphicDesign', 'vid...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2d animation</td>\n",
       "      <td>vedavasai</td>\n",
       "      <td>Totem Creative Mumbai based animation studio w...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>vasai, Mumbai</td>\n",
       "      <td>['vedavasai', 'animation', 'Studio', 'INFO']</td>\n",
       "      <td>positive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10397</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>JokerJokeE7</td>\n",
       "      <td>RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10398</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>AnimeICU</td>\n",
       "      <td>‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ AKEBIchan SonoBisqueDollWaKoiWoSuru Part...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['AKEBI_chan', 'SonoBisqueDollWaKoiWoSuru']</td>\n",
       "      <td>positive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>Ruttapak2</td>\n",
       "      <td>RT museacgth ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡πà‡∏≤...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['‡∏ö']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>Creamyyy_mm</td>\n",
       "      <td>RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>SineTST</td>\n",
       "      <td>RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...</td>\n",
       "      <td>th</td>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393848 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Keyword             User  \\\n",
       "1      2d animation        LinearUwu   \n",
       "2      2d animation    animationjobs   \n",
       "4      2d animation        l3oAlways   \n",
       "5      2d animation  BiraboneyeVicky   \n",
       "6      2d animation        vedavasai   \n",
       "...             ...              ...   \n",
       "10397   ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥      JokerJokeE7   \n",
       "10398   ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥         AnimeICU   \n",
       "10399   ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥        Ruttapak2   \n",
       "10400   ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥      Creamyyy_mm   \n",
       "10401   ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥          SineTST   \n",
       "\n",
       "                                                   Tweet Language        Time  \\\n",
       "1      RT blutack1966 The future will always be brigh...       en  2022-04-01   \n",
       "2      2D Background Artist job in Dicesol Animation ...       en  2022-04-01   \n",
       "4      RT irshema LGGGUYS ALWAYS TELL U SURE DEAL LET...       en  2022-04-01   \n",
       "5      RT irshema LGGGUYS ALWAYS TELL U SURE DEAL LET...       en  2022-04-01   \n",
       "6      Totem Creative Mumbai based animation studio w...       en  2022-04-01   \n",
       "...                                                  ...      ...         ...   \n",
       "10397  RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...       th  2022-03-30   \n",
       "10398  ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ AKEBIchan SonoBisqueDollWaKoiWoSuru Part...       th  2022-04-05   \n",
       "10399  RT museacgth ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡πà‡∏≤...       th  2022-04-03   \n",
       "10400  RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...       th  2022-04-03   \n",
       "10401  RT thisisoleang ‡∏™‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏µ‡πä‡∏î ‡πÄ‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ...       th  2022-03-29   \n",
       "\n",
       "       User Location                                            Hashtag  \\\n",
       "1                NaN                                                 []   \n",
       "2           Global üåç                                                 []   \n",
       "4         Poland üáµüá±   ['LGG', 'websitedesign', 'GraphicDesign', 'vid...   \n",
       "5                NaN  ['LGG', 'websitedesign', 'GraphicDesign', 'vid...   \n",
       "6      vasai, Mumbai       ['vedavasai', 'animation', 'Studio', 'INFO']   \n",
       "...              ...                                                ...   \n",
       "10397            NaN                                                 []   \n",
       "10398            NaN        ['AKEBI_chan', 'SonoBisqueDollWaKoiWoSuru']   \n",
       "10399            NaN                                              ['‡∏ö']   \n",
       "10400            NaN                                                 []   \n",
       "10401            NaN                                                 []   \n",
       "\n",
       "       Polarity  Likes  Retweet  Sentiment  \n",
       "1      positive    0.0     21.0   0.516667  \n",
       "2      positive    1.0      1.0   0.211039  \n",
       "4      positive    0.0     33.0   0.500000  \n",
       "5      positive    0.0     27.0   0.500000  \n",
       "6      positive    2.0      1.0   0.500000  \n",
       "...         ...    ...      ...        ...  \n",
       "10397   neutral    0.0    192.0   0.000000  \n",
       "10398  positive    5.0      1.0  66.670000  \n",
       "10399   neutral    0.0     10.0   0.000000  \n",
       "10400   neutral    0.0    192.0   0.000000  \n",
       "10401   neutral    0.0    192.0   0.000000  \n",
       "\n",
       "[393848 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DataManager()\n",
    "dm.unionfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏î‡∏π', '‡∏ó‡∏µ‡∏ß‡∏µ']\n",
      "['‡∏î‡∏π', '‡∏ó‡∏µ‡∏ß‡∏µ']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "text = \"‡∏î‡∏π‡∏ó‡∏µ‡∏ß‡∏µ\"\n",
    "list_word = word_tokenize(text)\n",
    "print(list_word)\n",
    "\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "stopwords = list(thai_stopwords())\n",
    "list_word_not_stopwords = [i for i in list_word if i not in stopwords]\n",
    "print(list_word_not_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "worddf = dm.collectwords(dm.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞</td>\n",
       "      <td>14814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>8700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∏î‡∏π</td>\n",
       "      <td>7752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∏ã‡∏µ</td>\n",
       "      <td>6312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bilibili</td>\n",
       "      <td>6165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‡πÄ‡∏Ñ‡πâ‡∏≤</td>\n",
       "      <td>6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‡πÇ‡∏Ñ</td>\n",
       "      <td>6075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‡∏™‡∏≤‡∏¢</td>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡∏ß‡∏≤‡∏¢</td>\n",
       "      <td>6017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡∏ü‡∏£‡∏µ</td>\n",
       "      <td>5987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•</td>\n",
       "      <td>5911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‡∏¢‡∏±‡∏á‡∏°‡∏µ</td>\n",
       "      <td>5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‡∏ô‡∏∏</td>\n",
       "      <td>5893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°</td>\n",
       "      <td>5883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>‡∏ô‡∏¥‡∏ß</td>\n",
       "      <td>5796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‡∏´‡∏±‡∏ß</td>\n",
       "      <td>5765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>‡∏•‡∏∏‡πâ‡∏ô</td>\n",
       "      <td>5730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‡∏Å‡∏£‡∏µ‡πä‡∏î</td>\n",
       "      <td>5704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thisisoleang</td>\n",
       "      <td>5701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‡∏™‡∏¥‡∏ó‡∏ò‡∏¥</td>\n",
       "      <td>5685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Count\n",
       "0         ‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞  14814\n",
       "1          ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥   8700\n",
       "2             ‡∏î‡∏π   7752\n",
       "3             ‡∏ã‡∏µ   6312\n",
       "4       Bilibili   6165\n",
       "5           ‡πÄ‡∏Ñ‡πâ‡∏≤   6084\n",
       "6             ‡πÇ‡∏Ñ   6075\n",
       "7            ‡∏™‡∏≤‡∏¢   6056\n",
       "8            ‡∏ß‡∏≤‡∏¢   6017\n",
       "9            ‡∏ü‡∏£‡∏µ   5987\n",
       "10        ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•   5911\n",
       "11         ‡∏¢‡∏±‡∏á‡∏°‡∏µ   5896\n",
       "12            ‡∏ô‡∏∏   5893\n",
       "13       ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°   5883\n",
       "14           ‡∏ô‡∏¥‡∏ß   5796\n",
       "15           ‡∏´‡∏±‡∏ß   5765\n",
       "16          ‡∏•‡∏∏‡πâ‡∏ô   5730\n",
       "17         ‡∏Å‡∏£‡∏µ‡πä‡∏î   5704\n",
       "18  thisisoleang   5701\n",
       "19         ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥   5685"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worddf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.deletekeyword(['anime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.collectfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "path=os.getcwd()\n",
    "shutil.rmtree(path+'//collectkeys//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dm.getrowwithkeys(['shounen']).sort_values('Keyword')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setnewDF(df1)\n",
    "dm.getperiod('2022-03-21','2022-03-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setdefaultDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in dm.collectword()[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dm.collectword()[:100],columns=['Word','Count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "dm.unionfile(filename)\n",
    "dm.formatdatetime('Time')\n",
    "print(dm.df['Time'].min().strftime('%Y/%m/%d'))\n",
    "print(dm.df['Time'].max().strftime('%Y/%m/%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['bl anime','anime comedy','anime romance','‡∏ï‡πà‡∏≤‡∏á‡πÇ‡∏•‡∏Å','anime','animation','shounen','pixar',\n",
    "        'harem','fantasy anime','sport anime','attack on titan','disney animation','animation studio',\n",
    "        'shounen ai','shoujo','‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞','2d animation','‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥','japan animation']\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "filename = ['tweet_data_2032022.csv','tweet_data_1932022.csv','tweet_data_2132022.csv','tweet_data_2232022.csv']\n",
    "i = 0\n",
    "for file in filename:\n",
    "    df1 = pd.read_csv(file)\n",
    "    if i != 0:\n",
    "        df = pd.concat([df,df1])\n",
    "        df.drop_duplicates(keep='last',inplace=True)\n",
    "    else:\n",
    "        df = df1\n",
    "        i += 1\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%Y/%m/%d')\n",
    "#df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%d %m %Y')\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df.sort_values(by=['Time','Keyword'],inplace=True)\n",
    "\n",
    "#time = list(set(df['Time'].tolist()))\n",
    "mask = (df['Time']>='2022-03-19') & (df['Time']<='2022-03-21')\n",
    "#mask = (df['Time']>='19-03-2022') & (df['Time']<='21-03-2022')\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"\"\n",
    "keyword = list(map(lambda x: x.lower(), keyword))\n",
    "keyword\n",
    "print(type(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞', 1007)\n",
      "('‡∏ó‡∏¥‡πâ‡∏á', 504)\n",
      "('‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢', 503)\n",
      "('‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', 456)\n",
      "('‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡∏±‡∏î', 392)\n",
      "('‡πÇ‡∏•‡∏Å', 377)\n",
      "('‡∏ï‡∏≠‡∏ô', 296)\n",
      "('‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ', 274)\n",
      "('‡∏Ñ‡∏ô', 254)\n",
      "('‡∏î‡∏π', 232)\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.corpus import thai_stopwords\n",
    "th_stopwords = list(thai_stopwords())\n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "filename = ['tweet_data_2732022.csv']\n",
    "i = 0\n",
    "for file in filename:\n",
    "    df1 = pd.read_csv(file)\n",
    "    if i != 0:\n",
    "        df = pd.concat([df,df1])\n",
    "        df.drop_duplicates(keep='last',inplace=True)\n",
    "    else:\n",
    "        df = df1\n",
    "        i += 1\n",
    "        \n",
    "dff = df.loc[df['Keyword'].isin(['‡∏ï‡πà‡∏≤‡∏á‡πÇ‡∏•‡∏Å','‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞'])]\n",
    "dff = dff.reset_index()##\n",
    "word = {}\n",
    "\n",
    "for index,row in dff.iterrows():##\n",
    "    if row['Language'] == 'en':\n",
    "        allwords = row['Tweet'].split()\n",
    "        for w in allwords: \n",
    "            if w not in en_stops:\n",
    "                if w in word:\n",
    "                    word[w] += 1\n",
    "                else:\n",
    "                    word[w] = 1\n",
    "    elif row['Language'] == 'th':   #\n",
    "        allwords = word_tokenize(row['Tweet'], engine='newmm')  #\n",
    "        for w in allwords: \n",
    "            if w not in th_stopwords:\n",
    "                if w in word:\n",
    "                    word[w] += 1\n",
    "                else:\n",
    "                    word[w] = 1\n",
    "    else:\n",
    "        pass\n",
    "del word['RT']\n",
    "del word[' ']\n",
    "sortword = sorted(word.items(),key=lambda x:x[1],reverse=True)\n",
    "for i in sortword[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.reset_index()\n",
    "for index,row in dff.iterrows():\n",
    "    print(row['Tweet'],row['Language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "all_words = ['There', 'is', 'a', 'tree','near','the','river']\n",
    "for word in all_words: \n",
    "    if word not in en_stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "url = \"https://api.aiforthai.in.th/ssense\"\n",
    " \n",
    "text = '‡∏™‡∏≤‡∏Ç‡∏≤‡∏ô‡∏µ‡πâ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ'\n",
    " \n",
    "data = {'text':text}\n",
    " \n",
    "headers = {\n",
    "    'Apikey': \"vIQAf35aRkc7QUbR1fTPvzvtkqtSKAaz\"\n",
    "    }\n",
    " \n",
    "response = requests.post(url, data=data, headers=headers)\n",
    " \n",
    "print(response.json())\n",
    "response.json()['preprocess']['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(set(df['Keyword'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "df = dm.unionfile(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Time\"] = pd.to_datetime(df[\"Time\"]).dt.strftime('%Y-%m-%d')\n",
    "keys = list(set(df['Keyword'].tolist()))\n",
    "folder = \"collectkeys\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "for key in keys:\n",
    "    path = str(folder+'/'+key)\n",
    "    dff = df.loc[df['Keyword'].isin([key])]\n",
    "    days = list(set(dff['Time'].tolist()))\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for d in days:\n",
    "        dfff = dff.loc[dff['Time'].isin([d])]\n",
    "        dfff.to_csv(path+'/'+key+'_'+d+'.csv',encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
